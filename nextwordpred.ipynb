{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install wikipedia","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T15:49:36.255060Z","iopub.execute_input":"2024-05-06T15:49:36.255402Z","iopub.status.idle":"2024-05-06T15:49:52.575148Z","shell.execute_reply.started":"2024-05-06T15:49:36.255374Z","shell.execute_reply":"2024-05-06T15:49:52.574099Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=309ac06a5c18c0909edebf202dee30e92315e6def555fa8f01dfedddd6e10895\n  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import wikipedia\n\n# Set a custom user agent to comply with Wikipedia's policy\nwikipedia.set_user_agent(\"My Wikipedia Scraping Bot (youremail@example.com)\")\n\n# List of Wikipedia page titles\npage_titles = [\"Climate Change\"]\n\n# Dictionary to store page titles and their content\ndocuments = {}\n\n# Number of pages to fetch for each document topic\nnum_pages_per_topic = 7\n\n# Fetch content for each page\nfor title in page_titles:\n    try:\n        # Fetch the page content\n        pages = wikipedia.search(title, results=num_pages_per_topic, suggestion=False)\n        \n        # Store the title and content in the dictionary\n        documents[title] = \"\\n\".join([wikipedia.page(page).content for page in pages])\n    except wikipedia.exceptions.PageError:\n        print(\"Page '{}' does not exist.\".format(title))\n    except wikipedia.exceptions.DisambiguationError as e:\n        print(\"Page '{}' is a disambiguation page. Skipping.\".format(title))\n\n# Print the fetched documents\nfor title, content in documents.items():\n    print(\"Title:\", title)\n    print(\"Content:\", content[:500])  # Print only the first 500 characters for demonstration\n    print(\"-------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:42:20.733917Z","iopub.execute_input":"2024-05-06T16:42:20.734801Z","iopub.status.idle":"2024-05-06T16:42:24.135906Z","shell.execute_reply.started":"2024-05-06T16:42:20.734764Z","shell.execute_reply":"2024-05-06T16:42:24.134949Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Title: Climate Change\nContent: In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global average temperature is more rapid than previous changes, and is primarily caused by humans burning fossil fuels. Fossil fuel use, deforestation, and some agricultural and industrial practices add to greenhouse gases, notably ca\n-------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"# Combine all the fetched content into a single corpus\ncorpus = \"\"\n\nfor title, content in documents.items():\n    corpus += content\n\n# Print the length of the corpus\nprint(\"Length of the corpus:\", len(corpus))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:42:25.974717Z","iopub.execute_input":"2024-05-06T16:42:25.975075Z","iopub.status.idle":"2024-05-06T16:42:25.980607Z","shell.execute_reply.started":"2024-05-06T16:42:25.975035Z","shell.execute_reply":"2024-05-06T16:42:25.979546Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Length of the corpus: 376052\n","output_type":"stream"}]},{"cell_type":"code","source":"#corpus = \"\"\"\"Artificial intelligence, or AI, is technology that enables computers and machines to simulate human intelligence and problem-solving capabilities.\n\nOn its own or combined with other technologies (e.g., sensors, geolocation, robotics) AI can perform tasks that would otherwise require human intelligence or intervention. Digital assistants, GPS guidance, autonomous vehicles, and generative AI tools (like Open AI's Chat GPT) are just a few examples of AI in the daily news and our daily lives.\n\nAs a field of computer science, artificial intelligence encompasses (and is often mentioned together with) machine learning and deep learning. These disciplines involve the development of AI algorithms, modeled after the decision-making processes of the human brain, that can ‘learn’ from available data and make increasingly more accurate classifications or predictions over time.\n\nArtificial intelligence has gone through many cycles of hype, but even to skeptics, the release of ChatGPT seems to mark a turning point. The last time generative AI loomed this large, the breakthroughs were in computer vision, but now the leap forward is in natural language processing (NLP). Today, generative AI can learn and synthesize not just human language but other data types including images, video, software code, and even molecular structures.\n\nApplications for AI are growing every day. But as the hype around the use of AI tools in business takes off, conversations around ai ethics and responsible ai become critically important. For more on where IBM stands on these issues, please read Building trust in AI. \nTypes of artificial intelligence: weak AI vs. strong AI\nWeak AI—also known as narrow AI or artificial narrow intelligence (ANI)—is AI trained and focused to perform specific tasks. Weak AI drives most of the AI that surrounds us today. \"Narrow\" might be a more apt descriptor for this type of AI as it is anything but weak: it enables some very robust applications, such as Apple's Siri, Amazon's Alexa, IBM watsonx™, and self-driving vehicles.\n\nStrong AI is made up of artificial general intelligence (AGI) and artificial super intelligence (ASI). AGI, or general AI, is a theoretical form of AI where a machine would have an intelligence equal to humans; it would be self-aware with a consciousness that would have the ability to solve problems, learn, and plan for the future. ASI—also known as superintelligence—would surpass the intelligence and ability of the human brain. While strong AI is still entirely theoretical with no practical examples in use today, that doesn't mean AI researchers aren't also exploring its development. In the meantime, the best examples of ASI might be from science fiction, such as HAL, the superhuman and rogue computer assistant in 2001: A Space Odyssey.\n\nDeep learning vs. machine learning\nMachine learning and deep learning are sub-disciplines of AI, and deep learning is a sub-discipline of machine learning.\n\nBoth machine learning and deep learning algorithms use neural networks to ‘learn’ from huge amounts of data. These neural networks are programmatic structures modeled after the decision-making processes of the human brain. They consist of layers of interconnected nodes that extract features from the data and make predictions about what the data represents.\n\nMachine learning and deep learning differ in the types of neural networks they use, and the amount of human intervention involved. Classic machine learning algorithms use neural networks with an input layer, one or two ‘hidden’ layers, and an output layer. Typically, these algorithms are limited to supervised learning: the data needs to be structured or labeled by human experts to enable the algorithm to extract features from the data.\n\nDeep learning algorithms use deep neural networks—networks composed of an input layer, three or more (but usually hundreds) of hidden layers, and an output layout. These multiple layers enable unsupervised learning: they automate extraction of features from large, unlabeled and unstructured data sets. Because it doesn’t require human intervention, deep learning essentially enables machine learning at scale. The rise of generative models\nGenerative AI refers to deep-learning models that can take raw data—say, all of Wikipedia or the collected works of Rembrandt—and “learn” to generate statistically probable outputs when prompted. At a high level, generative models encode a simplified representation of their training data and draw from it to create a new work that’s similar, but not identical, to the original data.\n\nGenerative models have been used for years in statistics to analyze numerical data. The rise of deep learning, however, made it possible to extend them to images, speech, and other complex data types. Among the first class of AI models to achieve this cross-over feat were variational autoencoders, or VAEs, introduced in 2013. VAEs were the first deep-learning models to be widely used for generating realistic images and speech.\n\n“VAEs opened the floodgates to deep generative modeling by making models easier to scale,” said Akash Srivastava, an expert on generative AI at the MIT-IBM Watson AI Lab. “Much of what we think of today as generative AI started here.”\n\nEarly examples of models, including GPT-3, BERT, or DALL-E 2, have shown what’s possible. In the future, models will be trained on a broad set of unlabeled data that can be used for different tasks, with minimal fine-tuning. Systems that execute specific tasks in a single domain are giving way to broad AI systems that learn more generally and work across domains and problems. Foundation models, trained on large, unlabeled datasets and fine-tuned for an array of applications, are driving this shift.\n\nAs to the future of AI, when it comes to generative AI, it is predicted that foundation models will dramatically accelerate AI adoption in enterprise. Reducing labeling requirements will make it much easier for businesses to dive in, and the highly accurate, efficient AI-driven automation they enable will mean that far more companies will be able to deploy AI in a wider range of mission-critical situations. For IBM, the hope is that the computing power of foundation models can eventually be brought to every enterprise in a frictionless hybrid-cloud environment.\n\nExplore foundation models in watsonx.ai\n\nArtificial intelligence applications\nThere are numerous, real-world applications for AI systems today. Below are some of the most common use cases:\n\nSpeech recognition\nAlso known as automatic speech recognition (ASR), computer speech recognition, or speech-to-text, speech recognition uses NLP to process human speech into a written format. Many mobile devices incorporate speech recognition into their systems to conduct voice search—Siri, for example—or provide more accessibility around texting in English or many widely-used languages. See how Don Johnston used IBM Watson Text to Speech to improve accessibility in the classroom with our case study.\n\nCustomer service\nOnline  virtual agents and chatbots are replacing human agents along the customer journey. They answer frequently asked questions (FAQ) around topics, like shipping, or provide personalized advice, cross-selling products or suggesting sizes for users, changing the way we think about customer engagement across websites and social media platforms. Examples include messaging bots on e-commerce sites with virtual agents , messaging apps, such as Slack and Facebook Messenger, and tasks usually done by virtual assistants and voice assistants. See how Autodesk Inc. used IBM watsonx Assistant to speed up customer response times by 99% with our case study.\n\nComputer vision\nThis AI technology enables computers and systems to derive meaningful information from digital images, videos and other visual inputs, and based on those inputs, it can take action. This ability to provide recommendations distinguishes it from image recognition tasks. Powered by convolutional neural networks, computer vision has applications within photo tagging in social media, radiology imaging in healthcare, and self-driving cars within the automotive industry. See how ProMare used IBM Maximo to set a new course for ocean research with our case study.\n\nSupply chain\nAdaptive robotics act on Internet of Things (IoT) device information, and structured and unstructured data to make autonomous decisions. NLP tools can understand human speech and react to what they are being told. Predictive analytics are applied to demand responsiveness, inventory and network optimization, preventative maintenance and digital manufacturing. Search and pattern recognition algorithms—which are no longer just predictive, but hierarchical—analyze real-time data, helping supply chains to react to machine-generated, augmented intelligence, while providing instant visibility and transparency. See how Hendrickson used IBM Sterling to fuel real-time transactions with our case study.\n\nWeather forecasting\nThe weather models broadcasters rely on to make accurate forecasts consist of complex algorithms run on supercomputers. Machine-learning techniques enhance these models by making them more applicable and precise. See how Emnotion used IBM Cloud to empower weather-sensitive enterprises to make more proactive, data-driven decisions with our case study.\n\nAnomaly detection\nAI models can comb through large amounts of data and discover atypical data points within a dataset. These anomalies can raise awareness around faulty equipment, human error, or breaches in security. See how Netox used IBM QRadar to protect digital businesses from cyberthreats with our case study.\n\nHistory of artificial intelligence: Key dates and names\nThe idea of \"a machine that thinks\" dates back to ancient Greece. But since the advent of electronic computing (and relative to some of the topics discussed in this article) important events and milestones in the evolution of artificial intelligence include the following:\n\n1950: Alan Turing publishes Computing Machinery and Intelligence (link resides outside ibm.com). In this paper, Turing—famous for breaking the German ENIGMA code during WWII and often referred to as the \"father of computer science\"— asks the following question: \"Can machines think?\"  From there, he offers a test, now famously known as the \"Turing Test,\" where a human interrogator would try to distinguish between a computer and human text response. While this test has undergone much scrutiny since it was published, it remains an important part of the history of AI, as well as an ongoing concept within philosophy as it utilizes ideas around linguistics.\n1956: John McCarthy coins the term \"artificial intelligence\" at the first-ever AI conference at Dartmouth College. (McCarthy would go on to invent the Lisp language.) Later that year, Allen Newell, J.C. Shaw, and Herbert Simon create the Logic Theorist, the first-ever running AI software program.\n1967: Frank Rosenblatt builds the Mark 1 Perceptron, the first computer based on a neural network that \"learned\" though trial and error. Just a year later, Marvin Minsky and Seymour Papert publish a book titled Perceptrons, which becomes both the landmark work on neural networks and, at least for a while, an argument against future neural network research projects.\n1980s: Neural networks which use a backpropagation algorithm to train itself become widely used in AI applications.\n1995: Stuart Russell and Peter Norvig publish Artificial Intelligence: A Modern Approach (link resides outside ibm.com), which becomes one of the leading textbooks in the study of AI. In it, they delve into four potential goals or definitions of AI, which differentiates computer systems on the basis of rationality and thinking vs. acting.\n\n1997: IBM's Deep Blue beats then world chess champion Garry Kasparov, in a chess match (and rematch).\n\n2004: John McCarthy writes a paper, What Is Artificial Intelligence? (link resides outside ibm.com), and proposes an often-cited definition of AI.\n2011: IBM Watson beats champions Ken Jennings and Brad Rutter at Jeopardy!\n2015: Baidu's Minwa supercomputer uses a special kind of deep neural network called a convolutional neural network to identify and categorize images with a higher rate of accuracy than the average human.\n2016: DeepMind's AlphaGo program, powered by a deep neural network, beats Lee Sodol, the world champion Go player, in a five-game match. The victory is significant given the huge number of possible moves as the game progresses (over 14.5 trillion after just four moves!). Later, Google purchased DeepMind for a reported USD 400 million.\n2023: A rise in large language models, or LLMs, such as ChatGPT, create an\nenormous change in performance of AI and its potential to drive enterprise value. With these new generative AI practices, deep-learning models can be pre-trained on vast amounts of raw, unlabeled data.\n\"\"\" ","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:31:24.856576Z","iopub.execute_input":"2024-05-06T16:31:24.856962Z","iopub.status.idle":"2024-05-06T16:31:24.873286Z","shell.execute_reply.started":"2024-05-06T16:31:24.856934Z","shell.execute_reply":"2024-05-06T16:31:24.872337Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\n# Text cleaning: Remove unwanted characters and symbols\ncleaned_corpus = re.sub(r'[^a-zA-Z\\s]', '', corpus)\n\n# Tokenization: Tokenize the cleaned corpus into words\ntokens = word_tokenize(cleaned_corpus)\n\n# Lowercasing\ntokens = [token.lower() for token in tokens]\n\n# Stopword removal\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [token for token in tokens if token not in stop_words]\n\n# Filtering rare words\nword_counts = Counter(filtered_tokens)\nmin_word_frequency = 5\nfiltered_tokens = [token for token in filtered_tokens if word_counts[token] >= min_word_frequency]\n\n# Limiting vocabulary size\nmax_vocab_size = 10000\nvocab = sorted(word_counts, key=word_counts.get, reverse=True)[:max_vocab_size]\n\n# Replace tokens with <unk> for out-of-vocabulary tokens\nfiltered_tokens = [token if token in vocab else '<unk>' for token in filtered_tokens]\n\n# Print the first few tokens after preprocessing\nprint(\"First 10 tokens after preprocessing:\", filtered_tokens[:10])\nprint(\"Total number of tokens after preprocessing:\", len(filtered_tokens))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:42:27.836970Z","iopub.execute_input":"2024-05-06T16:42:27.837820Z","iopub.status.idle":"2024-05-06T16:42:28.453609Z","shell.execute_reply.started":"2024-05-06T16:42:27.837779Z","shell.execute_reply":"2024-05-06T16:42:28.452625Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"First 10 tokens after preprocessing: ['common', 'usage', 'climate', 'change', 'describes', 'global', 'increase', 'global', 'average', 'effects']\nTotal number of tokens after preprocessing: 26321\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the sequence length (number of characters in each sequence)\nsequence_length = 150\n\n# Create input-output pairs\nsequences = []\nnext_chars = []\n\nfor i in range(0, len(tokens) - sequence_length):\n    # Extract input sequence and target character\n    seq = tokens[i:i + sequence_length]\n    target = tokens[i + sequence_length]\n    \n    # Add the input sequence and target character to the lists\n    sequences.append(seq)\n    next_chars.append(target)\n\nprint(\"Number of sequences:\", len(sequences))\n\n# Print the first few input-output pairs for demonstration\nfor i in range(3):\n    print(\"Input Sequence:\", sequences[i])\n    print(\"Target Character:\", next_chars[i])\n    print(\"--------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:42:39.503517Z","iopub.execute_input":"2024-05-06T16:42:39.504210Z","iopub.status.idle":"2024-05-06T16:42:39.665434Z","shell.execute_reply.started":"2024-05-06T16:42:39.504175Z","shell.execute_reply":"2024-05-06T16:42:39.664531Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Number of sequences: 54685\nInput Sequence: ['in', 'common', 'usage', 'climate', 'change', 'describes', 'global', 'warmingthe', 'ongoing', 'increase', 'in', 'global', 'average', 'temperatureand', 'its', 'effects', 'on', 'earths', 'climate', 'system', 'climate', 'change', 'in', 'a', 'broader', 'sense', 'also', 'includes', 'previous', 'longterm', 'changes', 'to', 'earths', 'climate', 'the', 'current', 'rise', 'in', 'global', 'average', 'temperature', 'is', 'more', 'rapid', 'than', 'previous', 'changes', 'and', 'is', 'primarily', 'caused', 'by', 'humans', 'burning', 'fossil', 'fuels', 'fossil', 'fuel', 'use', 'deforestation', 'and', 'some', 'agricultural', 'and', 'industrial', 'practices', 'add', 'to', 'greenhouse', 'gases', 'notably', 'carbon', 'dioxide', 'and', 'methane', 'greenhouse', 'gases', 'absorb', 'some', 'of', 'the', 'heat', 'that', 'the', 'earth', 'radiates', 'after', 'it', 'warms', 'from', 'sunlight', 'larger', 'amounts', 'of', 'these', 'gases', 'trap', 'more', 'heat', 'in', 'earths', 'lower', 'atmosphere', 'causing', 'global', 'warming', 'climate', 'change', 'has', 'an', 'increasingly', 'large', 'impact', 'on', 'the', 'environment', 'deserts', 'are', 'expanding', 'while', 'heat', 'waves', 'and', 'wildfires', 'are', 'becoming', 'more', 'common', 'amplified', 'warming', 'in', 'the', 'arctic', 'has', 'contributed', 'to', 'thawing', 'permafrost', 'retreat', 'of', 'glaciers', 'and', 'sea', 'ice', 'decline', 'higher', 'temperatures', 'are', 'also', 'causing']\nTarget Character: more\n--------------------\nInput Sequence: ['common', 'usage', 'climate', 'change', 'describes', 'global', 'warmingthe', 'ongoing', 'increase', 'in', 'global', 'average', 'temperatureand', 'its', 'effects', 'on', 'earths', 'climate', 'system', 'climate', 'change', 'in', 'a', 'broader', 'sense', 'also', 'includes', 'previous', 'longterm', 'changes', 'to', 'earths', 'climate', 'the', 'current', 'rise', 'in', 'global', 'average', 'temperature', 'is', 'more', 'rapid', 'than', 'previous', 'changes', 'and', 'is', 'primarily', 'caused', 'by', 'humans', 'burning', 'fossil', 'fuels', 'fossil', 'fuel', 'use', 'deforestation', 'and', 'some', 'agricultural', 'and', 'industrial', 'practices', 'add', 'to', 'greenhouse', 'gases', 'notably', 'carbon', 'dioxide', 'and', 'methane', 'greenhouse', 'gases', 'absorb', 'some', 'of', 'the', 'heat', 'that', 'the', 'earth', 'radiates', 'after', 'it', 'warms', 'from', 'sunlight', 'larger', 'amounts', 'of', 'these', 'gases', 'trap', 'more', 'heat', 'in', 'earths', 'lower', 'atmosphere', 'causing', 'global', 'warming', 'climate', 'change', 'has', 'an', 'increasingly', 'large', 'impact', 'on', 'the', 'environment', 'deserts', 'are', 'expanding', 'while', 'heat', 'waves', 'and', 'wildfires', 'are', 'becoming', 'more', 'common', 'amplified', 'warming', 'in', 'the', 'arctic', 'has', 'contributed', 'to', 'thawing', 'permafrost', 'retreat', 'of', 'glaciers', 'and', 'sea', 'ice', 'decline', 'higher', 'temperatures', 'are', 'also', 'causing', 'more']\nTarget Character: intense\n--------------------\nInput Sequence: ['usage', 'climate', 'change', 'describes', 'global', 'warmingthe', 'ongoing', 'increase', 'in', 'global', 'average', 'temperatureand', 'its', 'effects', 'on', 'earths', 'climate', 'system', 'climate', 'change', 'in', 'a', 'broader', 'sense', 'also', 'includes', 'previous', 'longterm', 'changes', 'to', 'earths', 'climate', 'the', 'current', 'rise', 'in', 'global', 'average', 'temperature', 'is', 'more', 'rapid', 'than', 'previous', 'changes', 'and', 'is', 'primarily', 'caused', 'by', 'humans', 'burning', 'fossil', 'fuels', 'fossil', 'fuel', 'use', 'deforestation', 'and', 'some', 'agricultural', 'and', 'industrial', 'practices', 'add', 'to', 'greenhouse', 'gases', 'notably', 'carbon', 'dioxide', 'and', 'methane', 'greenhouse', 'gases', 'absorb', 'some', 'of', 'the', 'heat', 'that', 'the', 'earth', 'radiates', 'after', 'it', 'warms', 'from', 'sunlight', 'larger', 'amounts', 'of', 'these', 'gases', 'trap', 'more', 'heat', 'in', 'earths', 'lower', 'atmosphere', 'causing', 'global', 'warming', 'climate', 'change', 'has', 'an', 'increasingly', 'large', 'impact', 'on', 'the', 'environment', 'deserts', 'are', 'expanding', 'while', 'heat', 'waves', 'and', 'wildfires', 'are', 'becoming', 'more', 'common', 'amplified', 'warming', 'in', 'the', 'arctic', 'has', 'contributed', 'to', 'thawing', 'permafrost', 'retreat', 'of', 'glaciers', 'and', 'sea', 'ice', 'decline', 'higher', 'temperatures', 'are', 'also', 'causing', 'more', 'intense']\nTarget Character: storms\n--------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM, Dropout\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, SimpleRNN\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert tokens to integers\ntoken_to_int = {token: i for i, token in enumerate(set(tokens))}\nint_to_token = {i: token for token, i in token_to_int.items()}\n\n# Convert sequences and next_chars to integers\nX = np.array([[token_to_int[token] for token in seq] for seq in sequences])\ny = np.array([token_to_int[token] for token in next_chars])\n\n# One-hot encode the target variable\ny_one_hot = to_categorical(y, num_classes=len(token_to_int))\n\n\nmodel = Sequential([\n    SimpleRNN(512, input_shape=(sequence_length, 1), return_sequences=True),\n    SimpleRNN(512, return_sequences=True),\n    SimpleRNN(512, return_sequences=True),\n    SimpleRNN(512),\n    Dropout(0.5),  # Adding dropout for regularization\n    Dense(len(token_to_int), activation='softmax')\n])\n\n# Compile the model\n#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nfrom tensorflow.keras.optimizers import Adamax\nmodel.compile(optimizer=Adamax(), loss='categorical_crossentropy', metrics=['accuracy'])\n\nX = np.reshape(X, (X.shape[0], sequence_length, 1))\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:56:53.463984Z","iopub.execute_input":"2024-05-06T16:56:53.464607Z","iopub.status.idle":"2024-05-06T16:56:55.346146Z","shell.execute_reply.started":"2024-05-06T16:56:53.464575Z","shell.execute_reply":"2024-05-06T16:56:55.345208Z"},"trusted":true},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_12\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_48 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m263,168\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_49 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_50 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_51 (\u001b[38;5;33mSimpleRNN\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │     \u001b[38;5;34m3,217,536\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,217,536</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,055,104\u001b[0m (19.28 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,055,104</span> (19.28 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,055,104\u001b[0m (19.28 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,055,104</span> (19.28 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, SimpleRNN\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert tokens to integers\ntoken_to_int = {token: i for i, token in enumerate(set(tokens))}\nint_to_token = {i: token for token, i in token_to_int.items()}\n\n# Convert sequences and next_chars to integers\nX = np.array([[token_to_int[token] for token in seq] for seq in sequences])\ny = np.array([token_to_int[token] for token in next_chars])\n\n# One-hot encode the target variable\ny_one_hot = to_categorical(y, num_classes=len(token_to_int))\n\n# Define the RNN model\nmodel = Sequential([\n    SimpleRNN(128, input_shape=(sequence_length, 1)),\n    Dense(len(token_to_int), activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Reshape input data for RNN\nX = np.reshape(X, (X.shape[0], sequence_length, 1))\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T21:24:04.529020Z","iopub.execute_input":"2024-05-05T21:24:04.529418Z","iopub.status.idle":"2024-05-05T21:24:05.887863Z","shell.execute_reply.started":"2024-05-05T21:24:04.529386Z","shell.execute_reply":"2024-05-05T21:24:05.886960Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,640\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4954\u001b[0m)           │       \u001b[38;5;34m639,066\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4954</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">639,066</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m655,706\u001b[0m (2.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">655,706</span> (2.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m655,706\u001b[0m (2.50 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">655,706</span> (2.50 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(X, y_one_hot, batch_size=128, epochs=100, validation_split=0.3)\n\n# Plot training history\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:56:59.082917Z","iopub.execute_input":"2024-05-06T16:56:59.083290Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1715014630.451737      98 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1715014630.542030      98 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1715014630.585164      98 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1715014630.625609      98 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.0426 - loss: 7.2526","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1715014684.023686     100 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 196ms/step - accuracy: 0.0427 - loss: 7.2519 - val_accuracy: 0.0384 - val_loss: 6.9618\nEpoch 2/100\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 172ms/step - accuracy: 0.0434 - loss: 6.8691 - val_accuracy: 0.0525 - val_loss: 7.0261\nEpoch 3/100\n\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.0470 - loss: 6.8850","output_type":"stream"}]},{"cell_type":"code","source":"# Function to generate text\ndef generate_text(seed_text, next_chars=100):\n    # Tokenize the seed text\n    seed_tokens = [token_to_int[token] for token in seed_text if token in token_to_int]\n    \n    # Generate text using the model\n    generated_text = seed_text\n    for i in range(next_chars):\n        # Reshape seed tokens for prediction\n        x_pred = np.reshape(seed_tokens, (1, len(seed_tokens), 1))\n        # Predict the next token\n        preds = model.predict(x_pred, verbose=0)[0]\n        # Convert predicted token to character\n        next_index = np.argmax(preds)\n        next_token = int_to_token[next_index]\n        # Append the predicted character to the generated text\n        generated_text += next_token\n        # Update seed tokens for next prediction\n        seed_tokens.append(next_index)\n        seed_tokens = seed_tokens[1:]\n    \n    return generated_text\n\n# Generate text using the model\nseed_text = \"artificial intelligence\"\ngenerated_text = generate_text(seed_text)\nprint(\"Generated Text:\")\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T16:41:43.723205Z","iopub.execute_input":"2024-05-06T16:41:43.723577Z","iopub.status.idle":"2024-05-06T16:41:49.864394Z","shell.execute_reply.started":"2024-05-06T16:41:43.723552Z","shell.execute_reply":"2024-05-06T16:41:49.863478Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Generated Text:\nartificial intelligencetoforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforforfor\n","output_type":"stream"}]}]}